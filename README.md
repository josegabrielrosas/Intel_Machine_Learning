# Intel Machine Learning
This course from Intel, provides an overview of machine learning fundamentals on modern Intel® architecture. Topics covered include:  
<br>Reviewing the types of problems that can be solved 
<br>Understanding building blocks Learning the fundamentals of building models in machine learning
<br>Exploring key algorithms
<br><br>By the end of this course, students will have practical knowledge of:  
<br>Supervised learning algorithms 
<br>Key concepts like under- and over-fitting, regularization, and cross-validation 
<br>How to identify the type of problem to be solved, choose the right algorithm, tune parameters, and validate a model 
<br><br>The course is structured around 12 weeks of lectures and exercises. 
<br>Each week requires three hours to complete. The exercises are implemented in Python*, so familiarity with the language is encouraged (you can learn along the way).
<br><br>I'm just copying what INTEL has in its website, all of its content is entirely property and developed by Intel
<br>
https://software.intel.com/content/www/us/en/develop/training/course-machine-learning.html
<br><br>
 
## Week 1
<dl>
This class introduces the basic data science toolset:

  <dd>Jupyter* Notebook for interactive coding</dd>
<dd>NumPy, SciPy, and pandas for numerical computation</dd>
<dd>Matplotlib and seaborn for data visualization</dd>
<dd>Scikit-learn* for machine learning libraries</dd>
You’ll use these tools to work through the exercises each week.

</dl>

 
## Week 2

This class introduces the basic concepts and vocabulary of machine learning:

 <dd>Supervised learning and how it can be applied to regression and classification problems</dd>
<dd>K-Nearest Neighbor (KNN) algorithm for classification</dd>

 
## Week 3

This class reviews the principles of core model generalization:

 <dd>The difference between over-fitting and under-fitting a model</dd>
<dd>Bias-variance tradeoffs</dd>
<dd>Finding the optimal training and test data set splits, cross-validation, and model complexity versus error</dd>
<dd>Introduction to the linear regression model for supervised learning</dd>

 
## Week 4

This class builds on concepts taught in previous weeks. Additionally you will:

 <dd>Learn about cost functions, regularization, feature selection, and hyper-parameters</dd>
<dd>Understand more complex statistical optimization algorithms like gradient descent and its application to linear regression</dd>

 
## Week 5

This class discusses the following:

 <dd>Logistic regression and how it differs from linear regression</dd>
<dd>Metrics for classification errors and scenarios in which they can be used</dd>

 
## Week 6

During this session, we review:

<dd>The basics of probability theory and its application to the Naïve Bayes classifier</dd>
<dd>The different types of Naïve Bayes classifiers and how to train a model using this algorithm</dd>

 
## Week 7

This week covers:

<dd>Support vector machines (SVMs)—a popular algorithm used for classification problems</dd>
<dd>Examples to learn SVM similarity to logistic regression</dd>
<dd>How to calculate the cost function of SVMs</dd>
<dd>Regularization in SVMs and some tips to obtain non-linear classifications with SVMs</dd>

 
## Week 8

Continuing with the topic of advanced supervised learning algorithms, this class covers:

<dd>Decision trees and how to use them for classification problems</dd>
<dd>How to identify the best split and the factors for splitting</dd>
<dd>Strengths and weaknesses of decision trees</dd>
<dd>Regression trees that help with classifying continuous values</dd>
 
## Week 9

Following on what was learned in Week 8, this class teaches:

<dd>The concepts of bootstrapping and aggregating (commonly known as “bagging”) to reduce variance</dd>
<dd>The Random Forest algorithm that further reduces the correlation seen in bagging models</dd>

## Week 10

This week, learn about the boosting algorithm that helps reduce variance and bias.
 
## Week 11

So far, the course has been heavily focused on supervised learning algorithms. This week, learn about unsupervised learning algorithms and how they can be applied to clustering and dimensionality reduction problems.

## Week 12

Dimensionality refers to the number of features in the dataset. Theoretically, more features should mean better models, but this is not true in practice. Too many features could result in spurious correlations, more noise, and slower performance. This week, learn algorithms that can be used to achieve a reduction in dimensionality, such as:

<dd>Principal Component Analysis (PCA)</dd>
<dd>Multidimensional Scaling (MDS)</dd>
